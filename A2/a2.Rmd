---
title: "Building prediction models for Airbnb bussiness in Rome"
author: "Zsiros, Gabriella, Asipovich, Hanna"
date: "2023-02-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#Download libraries
library(tidyverse)
library(ggplot2)
library(modelsummary)
library(kableExtra)
library(stargazer)
library(caret)
library(data.table)
library(fixest)
library(corrplot)
library(reshape2)
if(!require(geosphere)){
  install.packages("geosphere")
  library(geosphere)
}
library(ranger)
```

## Introduction

In this assignment we use the airbnb dataset from: http://insideairbnb.com/get-the-data.html We chose the city of Rome with more than 24,000 number of observations. We built 3 models to predict the price of accommodation per night for a midsize apartment and discuss their performance.

### Data exploration

As we explored the datatable, it contains various types of accomodation in Rome. We decided to narrow down our inquiry to apartments by room_type as 'Entire home/apt.' We also make sure to filter the data for the place to accomodate at least 2 people and not more than 6. We manipulate the data on neighborhoods, so that districts('neighbourhood_cleansed') are represented as factor. We further explore the data with datasummaries to identify our possible independet variables of interest. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Download data
listings <- read.csv('listings.csv')
```

We are looking at the different variables that can be later used as predictors. One of the important drives was to clean the data in a way that results in numerical variables. One of the challenges was the category of bathrooms, where we found values as 0, "half" bathrooms or empty data. 
 
Number of minimum nights is also an interesting feature where we could suspect to have outliers with values of more hundred, as minimum number of nights that can be booked. Long-term renting of Airbnbs might skew our findings and is not likely to be relevant to the business case. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#amenities <- unique(listings$amenities)
unique(listings$bathrooms_text)
unique(listings$accommodates)
unique(listings$bathrooms)           
summary(listings$price)
unique(listings$minimum_nights)
table(listings$minimum_nights)
listings$minimum_night
unique(sort(listings$bedrooms))
```

We filtered out relevant type of accommodation, as we are only interested in separate apartments for 2-6 people.

```{r message=FALSE, warning=FALSE, include=FALSE}
apartments<-listings %>% filter(room_type=="Entire home/apt") %>% filter(accommodates>=2)%>% filter(accommodates<=6) #14744 observations
```

Based on our initial exploration and domain knowledge, we kept the following variables to work with: id, neighbourhood, latitude,longitude, no. of people to accommodate, no. bathrooms, beds, various amenities,price, and minimum_nights.

```{r message=FALSE, warning=FALSE, include=FALSE}
#choosing interesting variables for the datatable and further exploration
dt<- apartments %>% select(
  id,
  neighbourhood=neighbourhood_cleansed,
  latitude,
  longitude,
  accommodates,
  bathrooms_text,
  beds,
  amenities,
  price,
  minimum_nights)

```

We cleaned up the district names and factorized them. 

```{r message=FALSE, warning=FALSE, include=FALSE}

#Cleaning data on districts
dt$neighbourhood <- sub(" .*", "", dt$neighbourhood)
dt$neighbourhood <- as.factor(dt$neighbourhood)

fct_count(dt$neighbourhood, sort = T, prop = T) # 56% is in city center
```


When it came to the decision about the bathroom categorization, we came to an agreement that we disregard whether it says private or not, since we already filtered for entire homes. 

Large number of bathrooms will also fall out as we get rid of outliers we deem not needed. Missing values were replaced with the median (some NAs were introduced by coercing to number), and the few 0 values were replaced with 1, with the reasoning that they were likely to be errors as most homes for short term rent have bathroom (1 happens to be the median value anyway)

```{r message=FALSE, warning=FALSE, include=FALSE}
#Transforming data on available bathrooms in the property into integer

table(dt$bathrooms_text)

dt <- dt %>% mutate(bathrooms_num = as.integer(gsub("[a-zA-Z\\s]+", "",
                                                    dt$bathrooms_text)),
                    .after= bathrooms_text )
dt <- dt %>%
  mutate(
    bathrooms_num = ifelse(is.na(dt$bathrooms_num),
                           median(dt$bathrooms_num, na.rm = T),
                           ifelse(bathrooms_num == 0,1,bathrooms_num)
    ), .after = bathrooms_num
  )

```

Price was transformed into a numeric value and we introduces some important dummies for certain amenities, such as Wifi, free parking and air conditioning.

When creating the dummy variables, we took a look at the amenities string dump and with come data cleaning on the text we set aside the amenties that are provided in the listings on a frequent basis, then decided based on real life experiece, what could be a price-affecting factor.

```{r message=FALSE, warning=FALSE, include=FALSE}
#Transform price into numeric
dt$price <- as.numeric(gsub("\\$", "", dt$price))

#Introduce dummies for selected amenities on wifi, free parking and air conditioning.
dt$wifi <- ifelse(grepl("Wifi", dt$amenities,ignore.case = TRUE), 1, 0)
dt$free_parking <- ifelse(grepl("free.*parking", dt$amenities,ignore.case = TRUE), 1, 0)
dt$ac <- ifelse(grepl("air.*conditioning", dt$amenities,ignore.case = TRUE), 1, 0)
dt$longterm <- ifelse(grepl("long.*term", dt$amenities,ignore.case = TRUE), 1, 0)
dt$kitchen <- ifelse(grepl("*Kitchen", dt$amenities,ignore.case = TRUE), 1, 0)
dt$TV <- ifelse(grepl("*TV*", dt$amenities,ignore.case = TRUE), 1, 0)
dt$coffeemaker <- ifelse(grepl("*Coffee.*maker", dt$amenities,ignore.case = TRUE), 1, 0)
dt$selfcheckin <- ifelse(grepl("Self.*check-in", dt$amenities,ignore.case = TRUE), 1, 0)
dt$balcony <- ifelse(grepl("*balcony*", dt$amenities,ignore.case = TRUE), 1, 0)
dt$micro <- ifelse(grepl("*microwave", dt$amenities,ignore.case = TRUE), 1, 0)
dt$breakfast <- ifelse(grepl("Breakfast", dt$amenities,ignore.case = TRUE), 1, 0)

```

Further inspection on the price revealed some extreme values in the upper 5% percentile, which will be removed as those are likely to be luxury apartments.


```{r include = TRUE}
quantile(dt$price,c(.01,.05,.1,.25,.5,.75,.9,.95,.99), na.rm = T) 
sum(is.na(dt$price))

#Having observed the datasummary on price, we will nor consider extreme values of upper 5%(possibly luxury apartments)
dt <- subset(dt, price <= 269)

summary(dt$price)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#Deciding on further data cleaning based on short-term or long-term rental.


quantile(dt$minimum_nights,.95) #95% of the data is less or equal than 5 minimum nights

dt <- dt %>% filter(dt$minimum_nights <= 5)
boxplot(dt$price)
```

Missing price values are replaced by the mean. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Fill in the missing observations on price with the median
dt <- dt %>%
  mutate(
    price = ifelse(is.na(dt$price),mean(dt$price, na.rm = T), price))
```

Checking distribution of prices.

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.height= 4}
fig3<-ggplot(dt, aes(x=price)) +
  geom_density(color = 'purple',alpha=0.3, size = 1.2) +
  theme_bw() +
  theme(legend.position="top") +
  guides(fill=guide_legend(nrow=1)) +
  scale_x_continuous(labels = function (x) {
    paste(formatC(x,format = "d",big.mark =","),"EUR")
  })+
  labs(title = 'Price distribution', x = "Price", y = "")
fig3

```


### Data visualisations

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4}

#data visualisations
fig1<- ggplot(
  data = dt,
  aes(x = neighbourhood, y = price)) +
  geom_boxplot(
    aes(group = neighbourhood),
    linewidth = 0.5, width = 0.6, alpha = 0.8, na.rm=T,
    outlier.shape = NA)+
  labs(x = "Neighbourhood",y = "Price(in EUR)") +
  theme_bw()
fig1

```

With `geosphere` package we introduce  a numerical value that can represent the location of the apartment other than the factorized neighbourhood to be later used for the "distance to the center" feature.

```{r message=FALSE, warning=FALSE, include=FALSE}
#Loading data on the distance to the center 

long_cent <- 12.496366
lat_cent <- 41.902782

dt <- dt %>% mutate(dist = distHaversine(cbind(dt$longitude, dt$latitude), c(long_cent, lat_cent)) / 1000)

#now that we have distance, lat and long are not needed
 dt <- dt[, !names(dt) %in% c("latitude", "longitude")]
 
 
 dt$is_center <- ifelse(dt$neighbourhood == 'I', 1, 0)

```

There seems to be a trend with some possible outliers in a long distance. This can be explained by the municipal borders of Rome being at the sea coast, which is also a popular destination and where Fiumicino aiport is located. 


**(i wouldn't include the distance bin scatter)**


```{r echo=FALSE, message=FALSE, warning=FALSE,fig.height= 4}
fig2<- ggplot(dt, aes(x = dist, y=price)) +
  stat_summary_bin(bins = 100) +
  theme_bw() +
  theme(legend.position="top") +
  guides(fill=guide_legend(nrow=1)) +
  scale_x_continuous(labels = function (x) {
    paste(formatC(x,format = "d",big.mark =","),"km")
  })+
  labs(title = 'Distance')
fig2
```

A closer inspection on the data reveals that the majority of the data points are close to the city center with approximately half in the Centro Storico, first district.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4}
fig4<- ggplot(dt, aes(x = dist, y=price)) +
  geom_point(color = 'purple', alpha = 0.2) +
  geom_smooth(method =  loess, color = 'blue') +
  theme_bw() +
  theme(legend.position="top") +
  guides(fill=guide_legend(nrow=1)) +
  scale_x_continuous(labels = function (x) {
    paste(formatC(x,format = "d",big.mark =","),"km")
  })+
  labs(title = 'Price vs Distance from the city center')
fig4
```

### Correlation

```{r echo=FALSE, message=FALSE, warning=FALSE}
numeric_dt <- keep( dt, is.numeric )
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}
cormat <- round(cor(numeric_dt),2)
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)

cormat_price <- subset(melted_cormat, grepl("price", Var1) | grepl("price", Var2)) %>% 
  arrange(desc(value))



cormat<- ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1))+
  coord_fixed()+
  ggtitle("Correlation Matrix")
print(cormat)


cormat_p<- ggplot(data = cormat_price, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1))+
  coord_fixed()+
  ggtitle("Correlation Matrix")


```

Highest correlations: Bathroom numbers, accomodates, distance, is_center (|.21| - |.37|)
ac, free parking, wifi (|.16| - |.09|)
coffeemaker, balcony, breakfast, microwave (|.08| - |.04|)
longterm booking possible, minimum_nights, self checkin, kitchen. 

(some negatiev correlations does not seem reasonable)

### Holdout

We create a holdout set from 15% of our cleaned data table, resulting approximately 2000 observations for the holdout. 

```{r}
set.seed(1927)

train_indices <- as.integer(createDataPartition(dt$price, p = 0.85, list = FALSE))
data_train <- dt[train_indices, ]
data_holdout <- dt[-train_indices, ]

```


## Models

The predictive models of choice are OLS, LASSO and Random Forest. The OLS regression is run according to 4 models of increasing complexity. 

### 1. OLS

We prepared 4 OLS models with an increasing number of features included. The last model was chosen for further comparison based on best RMSE and BIC. To run the OLS, a 5-fold cross-validation approach was chosen as producing the best results. 

Highest correlations: Bathroom numbers, accomodates, distance (|.21| - |.37|)
ac, free parking, wifi (|.16| - |.09|)
coffeemaker, balcony, breakfast, microwave (|.08| - |.04|)
longterm booking possible, minimum_nights, self checkin, kitchen. 


```{r echo=FALSE, message=FALSE, warning=FALSE}

#variables: dist, accommodates, wifi, neighborhood, free_parking, ac

model1 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center) ##incl numerical var
model2 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac) 
model3 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac + coffeemaker + balcony + breakfast + micro) 
model4 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac + coffeemaker + balcony + breakfast + micro + longterm + minimum_nights + selfcheckin + kitchen)
model5 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac + coffeemaker + balcony + breakfast + micro + longterm + minimum_nights + selfcheckin + kitchen +
                       neighbourhood)

reg1 <- lm(model1, data = data_train)
reg2 <- lm(model2, data = data_train)
reg3 <- lm(model3, data = data_train)
reg4 <- lm(model4, data = data_train)
reg5 <- lm(model5, data = data_train)


models <- c("reg1", "reg2","reg3", "reg4","reg5")
```

Through the cross validations, we set the number of folds at 5.

```{r echo=FALSE, message=FALSE, warning=FALSE}

# set number of folds

k <- 5

set.seed(1927)
cv1 <- train(model1, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv2 <- train(model2, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv3 <- train(model3, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv4 <- train(model4, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv5 <- train(model5, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))

cv <- c("cv1", "cv2", "cv3", "cv4", "cv5")


modelsummary::msummary(list(reg1,reg2,reg3,reg4,reg5), stars = T)
```

```{r}
variables <- c(4,7,11,15,16)

ols_summary <- data.table(Model = integer(), Variables = integer(), RMSE = double(), MAE = double(), R2 = double(), BIC = double())
for(i in 1:5) {
  ols_summary <- rbind(ols_summary,  data.table(
    Model = paste("Model",i),
    Variables = variables[i],
    RMSE = get(cv[i])$results$RMSE,
    MAE = get(cv[i])$results$MAE,
    R2 = get(cv[i])$results$Rsquared,
    BIC = BIC(get(models[i]))
  ))
}

knitr::kable(ols_summary, caption = "OLS summary", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )
```




Our 5th OLS model has the lowest RMSE and will be taken further to be compared between LASSO and Random Forest to find the best predicting model.

```{r message=FALSE, warning=FALSE, include=FALSE}
### keep 4th model

best_ols <- ols_summary[5,]
```

### 2. Lasso

LASSO method is used to ensure a better fit and minimise the sum of squared residuals. It is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model. 

As our second model, we selected LASSO as it penalizes over fitting. 

```{r echo=FALSE, include = TRUE, message=FALSE, warning=FALSE}

# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = k)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(-1, 1, by = 0.05))

set.seed(1927)
lasso_model <- caret::train(
  model5,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  trControl = train_control,
  tuneGrid = tune_grid,
  na.action=na.exclude)

print(lasso_model$bestTune$lambda)




lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  


```

```{r echo=FALSE, message=FALSE, warning=FALSE}
## selecting the best model for metrics
lasso_cv_metrics <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda)


lasso_summary <- data.table(Model = integer(), Variables = integer(), RMSE = double(), MAE = double(), R2 = double(), BIC = double())
lasso_summary <-  data.table(
    Model = paste("lasso"),
    Variables = variables[5],
    RMSE = lasso_cv_metrics$RMSE,
    MAE = lasso_cv_metrics$MAE,
    R2 = lasso_cv_metrics$Rsquared,
    BIC = NA)

```

### 3. Random Forest

We later use a bagging algorithm Random Forest to further fine-tune our model. Random Forest tends to perform better than OLS with non-linear patterns in the data, as well as showed us better results on RMSE than LASSO.
```{r message=FALSE, warning=FALSE, include=FALSE}
#setting train control
train_control <- trainControl(method = "cv",
                              number = k,
                              verboseIter = FALSE)
```

Tuning is set with several `.mtry` options

```{r message=TRUE, warning=FALSE, include=FALSE}
set.seed(1927)
system.time({
  rf_model <- train(
    model4,
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = expand.grid(
      .mtry = c(4:15),
      .splitrule = "variance",
      .min.node.size = c(50)
    ),
    importance = "impurity"
  )
})
rf_model

```

Random forest best result has 7 predictors when selecting model4 formula, with 7 variables.

```{r echo=TRUE, message=FALSE, warning=FALSE}


 forest_summary <- data.table(Model = integer(), Variables = integer(), RMSE = double(), MAE = double(), R2 = double(), BIC = double())

forest_summary <-  data.table(
    Model = paste("Random forest"),
    Variables = rf_model$results$mtry,
   RMSE = rf_model$results$RMSE,
  MAE = rf_model$results$MAE,
  R2 = rf_model$results$Rsquared,
    BIC = NA)


knitr::kable( forest_summary, caption = "TITLE", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )

```

```{r message=FALSE, warning=FALSE, include=FALSE}
#leave 4th rf model, .mtry=7
best_forest<- forest_summary[2,]

```

We compare all three models to decide on the best one for our purposes of price setting for a mid-range apartment in Rome.

```{r echo=FALSE, message=FALSE, warning=FALSE}
diff_models <- c("best_ols", "lasso_summary", "best_forest")

summary_all <- data.table(Model = integer(), Variables = integer(), 
                          RMSE = double(), MAE = double(), R2 = double())

for (i in 1:length(diff_models)) {
  summary_all <-  rbind(summary_all, tibble(
  Model = diff_models[i],
  Variables = get(diff_models[i])$Variables,
  RMSE = get(diff_models[i])$RMSE,
  MAE = get(diff_models[i])$MAE,
  R2 = get(diff_models[i])$R2))}


summary_all$Model <- c("OLS","LASSO","Random Forest")
knitr::kable( summary_all, caption = "TITLE", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )
```

### Diagnostics

We test the performance of our final chosen model, Random Forest, on the holdout dataset. 



```{r}
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}
rf_model_var_imp <- ranger::importance(rf_model$finalModel)/1000

rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

print(rf_model_var_imp)
```


Out-of-sample performance: RMSE # mean(price) on the holdout set.
Our

```{r}

data_holdout <- data_holdout %>%
  mutate(predicted_price = predict(rf_model, newdata = data_holdout))

 

print(paste("RMSE value for the holdout dataset:",RMSE(data_holdout$predicted_price,data_holdout$price)))


```

```{r, fig.height= 4, fig.width= 4}
ggplot(data = data_holdout, aes(x = price)) +
  geom_point(aes(y = predicted_price), colour = "red") +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed", size = 1.2) +
  xlim(0, 300) +
  ylim(0, 300)

```


##Extra Task 

### External validity with different dates

Our analysed dataset comes from a specific date in 13-12-2022 with data going back to one year. What is also available however, is a booking data of future dates, where we can test if our prediction models can predict the prices for the future. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Loading and cleaning the calendar data
calendar <- read.csv("calendar.csv")
calendar <- calendar%>%
  rename(
    id = listing_id,
    price_extra=price)%>%
  select(id, date, price_extra)
calendar$price_extra <- as.numeric(gsub("\\$", "", calendar$price_extra))


calendar <- calendar %>%
  group_by(id) %>%
  summarise(mean = mean(price_extra))


#Joining the data tables
data_extra <- left_join(dt, calendar, by = "id")
data_extra <- data_extra %>% rename( future_price = mean)

data_extra <- data_extra %>% mutate(OLS_pred = predict(cv5, newdata = data_extra),
                                    Lasso_pred = predict(lasso_model,newdata = data_extra),
                                    RF_pred = predict(rf_model,newdata = data_extra))


```

After joining our analyzed datatable with the future booking data, we introduced a variable 'future price'. Future price has 409 NAs, which were replaced with mean value.

```{r message=TRUE, warning=FALSE, include=FALSE}
data_extra <- data_extra %>%
  mutate(
    future_price = ifelse(is.na(data_extra$future_price),
                          mean(data_extra$future_price, na.rm = T), 
                          future_price))

summary(data_extra$future_price) 
```

When summarizing the data we can see that the RMSE is a lot higher. The order of the model fit did not change, however, random forest performs better than the previous two, with Lasso having slightly better results than OLS. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
futureOLS <-  RMSE(data_extra$OLS_pred,data_extra$future_price)
futureLASSO <- RMSE(data_extra$Lasso_pred,data_extra$future_price)
futureRF <- RMSE(data_extra$RF_pred,data_extra$future_price)

valid <- data.frame(
  Model = summary_all$Model,
  Past_Data_RMSE = summary_all$RMSE,
  Future_Data_RMSE = c(futureOLS, futureLASSO, futureRF)
)



knitr::kable(valid, caption = "TITLE", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )
```

Future prices vs prediction with Random forest. 

What skews our data are the listings with high prices, which are not frequent but take on extreme values. 

We can argue that while for the accuracy of future prediction it could have been better to keep our outliers in the very beginning, but when it comes to the business case, and on how to position the new apartments on the market, being mid-sized apartment without any special feature to our knowledge, it is best to keep our 95% threshold. The skewness also means that the future listings will also have outliers that might not be relevant to our business case. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot( data= data_extra, aes(x= future_price))+
  geom_point(aes(y= RF_pred),colour = 'red')+
  geom_abline(intercept = 0, slope = 1,color = 'black' ,linetype = 'dashed')
```

When comparing the prediction models we can see that all of our prediction models overestimate the distribution and the frequency around the mean value. 

```{r message=FALSE, warning=FALSE, include=FALSE}

data_extra2 <- data_extra %>% select(price, future_price, OLS_pred,Lasso_pred,RF_pred)
# Reshape the data into a long format
data_long <- gather(data_extra2, key = "Series", value = "Price", OLS_pred, Lasso_pred, RF_pred, price, future_price)
```

Checking out the visual considering future price prediction.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plot the data using ggplot2
ggplot(data = data_long, aes(x = Price, color = Series)) +
  geom_density() +
  scale_color_manual(values = c("OLS_pred" = "orange", "Lasso_pred" = "blue", "RF_pred" = "red", "price" = "black", "future_price" = "green")) +
  labs(x = "", y = "Price") +
  theme_classic()
```

## APPENDIX

```{r}
print(cormat_price) 
```

```{r}
modelsummary::msummary(list(reg1,reg2,reg3,reg4,reg5), stars = T, output = "kableExtra")
```

```{r}
print(lasso_coeffs)
```

