---
title: "Building prediction models for Airbnb bussiness in Rome"
author: "Gabriella Zsiros , Hanna Asipovich"
date: "2023-02-07"
output:
  prettydoc::html_pretty:
theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
knitr::opts_chunk$set(fig.height = 4)
knitr::opts_chunk$set(fig.align = 'center')
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#Download libraries
library(tidyverse)
library(ggplot2)
library(modelsummary)
library(kableExtra)
library(stargazer)
library(caret)
library(data.table)
library(fixest)
library(corrplot)
library(reshape2)
if(!require(geosphere)){
  install.packages("geosphere")
  library(geosphere)
}
library(ranger)
```

## Introduction

In this assignment we use the airbnb dataset from: http://insideairbnb.com/get-the-data.html We chose the city of Rome with more than 24,000 number of observations. We built 3 models (OLS, LASSO, RF) to predict the price of accommodation per night for a midsize apartment and discuss their performance. For the complete code of the assignment see the Github repository: https://github.com/gabizsiros/DA3_Prediction/tree/main/A2

### Data exploration

As we explored the datatable, it contains various types of accommodation in Rome. We decided to narrow down our inquiry to apartments by room_type as `Entire home/apt`. We also make sure to filter the data for the place to accommodate at least 2 people and not more than 6. We manipulate the data on neighborhoods, so that districts(`neighbourhood_cleansed`) are represented as factor. We further explore the data with datasummaries to identify our possible independent variables of interest. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Download data
listings <- read.csv('listings.csv')
```

We are looking at the different variables that can be later used as predictors. One of the important drives was to clean the data in a way that results in numerical variables. One of the challenges was the category of bathrooms, where we found values as 0, "half" bathrooms or empty data. 
 
Number of minimum nights is also an interesting feature where we could suspect to have outliers with values above 100 as minimum number of nights that can be booked. Long-term renting of Airbnbs might skew our findings and is not likely to be relevant to the business case. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#amenities <- unique(listings$amenities)
unique(listings$bathrooms_text)
unique(listings$accommodates)
unique(listings$bathrooms)           
summary(listings$price)
unique(listings$minimum_nights)
table(listings$minimum_nights)
listings$minimum_night
unique(sort(listings$bedrooms))
```

We filtered out relevant type of accommodation, as we are only interested in separate apartments for 2-6 people.

```{r message=FALSE, warning=FALSE, include=FALSE}
apartments<-listings %>% filter(room_type=="Entire home/apt") %>% filter(accommodates>=2)%>% filter(accommodates<=6) #14744 observations
```

Based on our initial exploration and domain knowledge, we kept the following variables to work with: id, neighborhood, latitude,longitude, no. of people to accommodate, no. bathrooms, beds, various amenities,price, and minimum_nights.

```{r message=FALSE, warning=FALSE, include=FALSE}
#choosing interesting variables for the datatable and further exploration
dt<- apartments %>% select(
  id,
  neighbourhood=neighbourhood_cleansed,
  latitude,
  longitude,
  accommodates,
  bathrooms_text,
  beds,
  amenities,
  price,
  minimum_nights)

```

We cleaned up the district names and factorized them. 

```{r message=FALSE, warning=FALSE, include=FALSE}

#Cleaning data on districts
dt$neighbourhood <- sub(" .*", "", dt$neighbourhood)
dt$neighbourhood <- as.factor(dt$neighbourhood)

fct_count(dt$neighbourhood, sort = T, prop = T) # 56% is in city center
```
When it came to the decision about the bathroom categorization, we came to an agreement that we disregard whether it says private or not, since we already filtered for entire homes. 

Large number of bathrooms will also fall out as we get rid of outliers we deem not needed. Missing values were replaced with the median (some NAs were introduced by coercing to number), and the few 0 values were replaced with 1, with the reasoning that they were likely to be errors as most homes for short-term rent have bathroom (1 happens to be the median value, anyway).

```{r message=FALSE, warning=FALSE, include=FALSE}
#Transforming data on available bathrooms in the property into integer

table(dt$bathrooms_text)

dt <- dt %>% mutate(bathrooms_num = as.integer(gsub("[a-zA-Z\\s]+", "",
                                                    dt$bathrooms_text)),
                    .after= bathrooms_text )
dt <- dt %>%
  mutate(
    bathrooms_num = ifelse(is.na(dt$bathrooms_num),
                           median(dt$bathrooms_num, na.rm = T),
                           ifelse(bathrooms_num == 0,1,bathrooms_num)
    ), .after = bathrooms_num
  )

```

Price was transformed into a numeric value and we introduced some dummies for important amenities, such as Wifi, free parking and air conditioning.

When creating the dummy variables, we took a look at the amenities string dump and with come data cleaning on the text, we set aside the amenties that are provided in the listings on a frequent basis. Then we decided based on real life experiece, what could be a price-affecting factor and considered those.

```{r message=FALSE, warning=FALSE, include=FALSE}
#Transform price into numeric
dt$price <- as.numeric(gsub("\\$", "", dt$price))

#Introduce dummies for selected amenities on wifi, free parking and air conditioning.
dt$wifi <- ifelse(grepl("Wifi", dt$amenities,ignore.case = TRUE), 1, 0)
dt$free_parking <- ifelse(grepl("free.*parking", dt$amenities,ignore.case = TRUE), 1, 0)
dt$ac <- ifelse(grepl("air.*conditioning", dt$amenities,ignore.case = TRUE), 1, 0)
dt$longterm <- ifelse(grepl("long.*term", dt$amenities,ignore.case = TRUE), 1, 0)
dt$kitchen <- ifelse(grepl("*Kitchen", dt$amenities,ignore.case = TRUE), 1, 0)
dt$TV <- ifelse(grepl("*TV*", dt$amenities,ignore.case = TRUE), 1, 0)
dt$coffeemaker <- ifelse(grepl("*Coffee.*maker", dt$amenities,ignore.case = TRUE), 1, 0)
dt$selfcheckin <- ifelse(grepl("Self.*check-in", dt$amenities,ignore.case = TRUE), 1, 0)
dt$balcony <- ifelse(grepl("*balcony*", dt$amenities,ignore.case = TRUE), 1, 0)
dt$micro <- ifelse(grepl("*microwave", dt$amenities,ignore.case = TRUE), 1, 0)
dt$breakfast <- ifelse(grepl("Breakfast", dt$amenities,ignore.case = TRUE), 1, 0)

```

Further inspection on the price revealed some extreme values in the upper 5% percentile, which will be removed as those are likely to be luxury apartments.


```{r include = FALSE}

knitr::kable(quantile(dt$price,c(.01,.05,.1,.25,.5,.75,.9,.95,.99), na.rm = T), caption = "Price distribution", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' , full_width = F)
print(quantile(dt$price,c(.01,.05,.1,.25,.5,.75,.9,.95,.99), na.rm = T))

sum(is.na(dt$price))

#Having observed the datasummary on price, we will nor consider extreme values of upper 5%(possibly luxury apartments)
dt <- subset(dt, price <= 269)

summary(dt$price)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#Deciding on further data cleaning based on short-term or long-term rental.


quantile(dt$minimum_nights,.95) #95% of the data is less or equal than 5 minimum nights

dt <- dt %>% filter(dt$minimum_nights <= 5)
boxplot(dt$price)
```

Missing price values are replaced by the mean. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Fill in the missing observations on price with the median
dt <- dt %>%
  mutate(
    price = ifelse(is.na(dt$price),mean(dt$price, na.rm = T), price))
```

Checking distribution of prices.

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.height= 4}
fig3<-ggplot(dt, aes(x=price)) +
  geom_density(color = 'purple',alpha=0.3, size = 1.2) +
  theme_bw() +
  theme(legend.position="top") +
  guides(fill=guide_legend(nrow=1)) +
  scale_x_continuous(labels = function (x) {
    paste(formatC(x,format = "d",big.mark =","),"EUR")
  })+
  labs(title = 'Price distribution', x = "Price", y = "")
fig3

```

When exploring the price relationship with factors, we can see that the neighbourhood has an effect on price. 

```{r echo=FALSE, message=FALSE, warning=FALSE}

#data visualisations
fig1<- ggplot(
  data = dt,
  aes(x = neighbourhood, y = price)) +
  geom_boxplot(
    aes(group = neighbourhood), fill = 'lavender',
    linewidth = 0.5, width = 0.6, alpha = 0.8, na.rm=T,
    outlier.shape = NA)+
  labs(x = "Neighbourhood",y = "Price(in EUR)") +
  theme_bw()
fig1

```

With `geosphere` package we introduce  a numerical value that can represent the location of the apartment other than the factorized neighbourhood to be later used for the "distance to the center" feature. Considering that a large part of our listings is located in the first district, **Centro Storico**, we introduced a dummy variable to better facilitate CART based models. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Loading data on the distance to the center 

long_cent <- 12.496366
lat_cent <- 41.902782

dt <- dt %>% mutate(dist = distHaversine(cbind(dt$longitude, dt$latitude), c(long_cent, lat_cent)) / 1000)

#now that we have distance, lat and long are not needed
 dt <- dt[, !names(dt) %in% c("latitude", "longitude")]
 
 
 dt$is_center <- ifelse(dt$neighbourhood == 'I', 1, 0)

```

There seems to be a trend with some possible outliers in a long distance. This can be explained by the municipal borders of Rome being at the sea coast, which is also a popular destination and where Fiumicino aiport is located. 


**(i wouldn't include the distance bin scatter)**


```{r echo=FALSE, message=FALSE, warning=FALSE,fig.height= 4}
fig2<- ggplot(dt, aes(x = dist, y=price)) +
  stat_summary_bin(bins = 100) +
  theme_bw() +
  theme(legend.position="top") +
  guides(fill=guide_legend(nrow=1)) +
  scale_x_continuous(labels = function (x) {
    paste(formatC(x,format = "d",big.mark =","),"km")
  })+
  labs(title = 'Distance')
fig2
```

A closer inspection on the data reveals that the majority of the data points are close to the city center with approximately half in the Centro Storico, first district.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4}
fig4<- ggplot(dt, aes(x = dist, y=price)) +
  geom_point(color = 'purple', alpha = 0.2) +
  geom_smooth(method =  loess, color = 'blue') +
  theme_bw() +
  theme(legend.position="top") +
  guides(fill=guide_legend(nrow=1)) +
  scale_x_continuous(labels = function (x) {
    paste(formatC(x,format = "d",big.mark =","),"km")
  })+
  labs(title = 'Price vs Distance from the city center',x = "Distance", y= "Price")
fig4
```

### Correlation

```{r echo=FALSE, message=FALSE, warning=FALSE}
numeric_dt <- keep( dt, is.numeric )
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}
cormat <- round(cor(numeric_dt),2)
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)

cormat_price <- subset(melted_cormat, grepl("price", Var1) | grepl("price", Var2)) %>% 
  arrange(desc(value))

knitr::kable(cormat_price, caption = "Price correlations", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' , full_width = F)


cormat<- ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1))+
  coord_fixed()+
  ggtitle("Correlation Matrix")
print(cormat)


cormat_p<- ggplot(data = cormat_price, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1))+
  coord_fixed()+
  ggtitle("Correlation Matrix")


```

Highest correlations were shown by:  
Bathroom numbers, accomodates, distance, is_center (|.21| - |.37|)  
ac, free parking, wifi (|.16| - |.09|)  
coffeemaker, balcony, breakfast, microwave (|.08| - |.04|)  
longterm booking possible, minimum_nights, self check-in, kitchen. 

(some of negative correlations does not seem reasonable)


## Models

The predictive models of choice are OLS, LASSO and Random Forest. The OLS regression is run according to 5 models of increasing complexity. 

### Holdout

We created a holdout set as a random 15% of our cleaned data table, resulting in approximately 2,000 observations for the holdout. 

```{r}
set.seed(1927)

train_indices <- as.integer(createDataPartition(dt$price, p = 0.85, list = FALSE))
data_train <- dt[train_indices, ]
data_holdout <- dt[-train_indices, ]

```


### 1. OLS

We prepared 4 OLS models with an increasing number of features included. The last model was chosen for further comparison based on best RMSE and BIC. To run the OLS, a 5-fold cross-validation approach was chosen as producing the best results. 


```{r echo=FALSE, message=FALSE, warning=FALSE}

#variables: dist, accommodates, wifi, neighborhood, free_parking, ac

model1 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center) ##incl numerical var
model2 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac) 
model3 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac + coffeemaker + balcony + breakfast + micro) 
model4 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac + coffeemaker + balcony + breakfast + micro + longterm + minimum_nights + selfcheckin + kitchen)
model5 <- as.formula(price ~ dist + bathrooms_num + accommodates + is_center + wifi + free_parking + ac + coffeemaker + balcony + breakfast + micro + longterm + minimum_nights + selfcheckin + kitchen +
                       neighbourhood)

reg1 <- lm(model1, data = data_train)
reg2 <- lm(model2, data = data_train)
reg3 <- lm(model3, data = data_train)
reg4 <- lm(model4, data = data_train)
reg5 <- lm(model5, data = data_train)


models <- c("reg1", "reg2","reg3", "reg4","reg5")
```


Through the cross validations, we set the number of folds at 5.

```{r echo=FALSE, message=FALSE, warning=FALSE}

# set number of folds

k <- 5

set.seed(1927)
cv1 <- train(model1, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv2 <- train(model2, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv3 <- train(model3, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv4 <- train(model4, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(1927)
cv5 <- train(model5, data_train, method = "lm", trControl = trainControl(method = "cv", number = k))

cv <- c("cv1", "cv2", "cv3", "cv4", "cv5")


modelsummary::msummary(list(reg1,reg2,reg3,reg4,reg5), stars = T)
```

Our modelsummary confirms the significance of variables through correlations. Factorizing the neighborhoods seems to improve the performance. 

```{r}
variables <- c(4,7,11,15,16)

ols_summary <- data.table(Model = integer(), Variables = integer(), RMSE = double(), MAE = double(), R2 = double(), BIC = double())
for(i in 1:5) {
  ols_summary <- rbind(ols_summary,  data.table(
    Model = paste("Model",i),
    Variables = variables[i],
    RMSE = get(cv[i])$results$RMSE,
    MAE = get(cv[i])$results$MAE,
    R2 = get(cv[i])$results$Rsquared,
    BIC = BIC(get(models[i]))
  ))
}

knitr::kable(ols_summary, caption = "OLS summary", digits = 4 ) %>% kable_styling( position = "center", latex_options = 'hold_position' , full_width = F)
```




Our 5th OLS model has the lowest RMSE and will be taken further to be compared between LASSO and Random Forest to find the best predicting model.

```{r message=FALSE, warning=FALSE, include=FALSE}
### keep 5th model

best_ols <- ols_summary[5,]
```

### 2. Lasso

LASSO method is used to ensure a better fit and minimise the sum of squared residuals. It is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model. 

As our second model, we selected LASSO as it penalizes over-fitting. The model iterates through various values for lambda, with 0.05 steps between 0 and 1.

```{r echo=FALSE, include = TRUE, message=FALSE, warning=FALSE}

# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = k)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0, 1, by = 0.05))

set.seed(1927)
lasso_model <- caret::train(
  model5,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  trControl = train_control,
  tuneGrid = tune_grid,
  na.action=na.exclude)

print(paste("Best lambda in the various LASSO models: ",lasso_model$bestTune$lambda))




lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  


```

```{r echo=FALSE, message=FALSE, warning=FALSE}
## selecting the best model for metrics
lasso_cv_metrics <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda)


lasso_summary <- data.table(Model = integer(), Variables = integer(), RMSE = double(), MAE = double(), R2 = double(), BIC = double())
lasso_summary <-  data.table(
    Model = paste("lasso"),
    Variables = variables[5],
    RMSE = lasso_cv_metrics$RMSE,
    MAE = lasso_cv_metrics$MAE,
    R2 = lasso_cv_metrics$Rsquared,
    BIC = NA)

```

### 3. Random Forest

We later use a bagging algorithm Random Forest to further fine-tune our model. Random Forest tends to perform better than OLS with non-linear patterns in the data, as well as showed us better results on RMSE than LASSO. While OLS and LASSO could handle the neighborhood variables as factors, we decided to focus on the first district as a binary decision point when making the  regression trees. Here we take our model with the most vaiable, which does not contain the neighbourhoods as factors.

```{r message=FALSE, warning=FALSE, include=FALSE}
#setting train control
train_control <- trainControl(method = "cv",
                              number = k,
                              verboseIter = FALSE)
```

Tuning is set with several `.mtry` options, between 4 and 15.

```{r message=TRUE, warning=FALSE, include=FALSE}
set.seed(1927)
system.time({
  rf_model <- train(
    model4,
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = expand.grid(
      .mtry = c(4:15),
      .splitrule = "variance",
      .min.node.size = c(50)
    ),
    importance = "impurity"
  )
})
rf_model

```

Random forest's best result has 5 predictors when selecting model 4 formula (with 15 variables).

```{r echo=FALSE, message=FALSE, warning=FALSE}


 forest_summary <- data.table(Model = integer(), Variables = integer(), RMSE = double(), MAE = double(), R2 = double())

forest_summary <-  data.table(
    Model = paste("Random forest"),
    Variables = rf_model$results$mtry,
   RMSE = rf_model$results$RMSE,
  MAE = rf_model$results$MAE,
  R2 = rf_model$results$Rsquared)


knitr::kable( forest_summary, caption = "Random Forest summary", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position', full_width = F )

```

```{r message=FALSE, warning=FALSE, include=FALSE}
#leave 4th rf model, .mtry=7
best_forest<- forest_summary[2,]

```

We compare all three models to decide on the best one for our purposes of price setting for a mid-range apartment in Rome.

```{r echo=FALSE, message=FALSE, warning=FALSE}
diff_models <- c("best_ols", "lasso_summary", "best_forest")

summary_all <- data.table(Model = integer(), Variables = integer(), 
                          RMSE = double(), MAE = double(), R2 = double())

for (i in 1:length(diff_models)) {
  summary_all <-  rbind(summary_all, tibble(
  Model = diff_models[i],
  Variables = get(diff_models[i])$Variables,
  RMSE = get(diff_models[i])$RMSE,
  MAE = get(diff_models[i])$MAE,
  R2 = get(diff_models[i])$R2))}


summary_all$Model <- c("OLS","LASSO","Random Forest")
knitr::kable( summary_all, caption = "Model Summary", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position', full_width = F )
```

## Diagnostics

We test the performance of our final chosen model, Random Forest, on the holdout dataset. 
Variable importance shows that distance and location  has high importance, as well as the number of bathrooms and accommodation capacity, which indicate the size of the apartment, followed by the number of minimum nights required variable.


```{r}
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}
rf_model_var_imp <- ranger::importance(rf_model$finalModel)/1000

rf_model_var_imp_df <-
  data.frame(imp = rf_model_var_imp) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

knitr::kable(rf_model_var_imp_df, caption = "Variable importance", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position', full_width = F )
```

```{r}

data_holdout <- data_holdout %>%
  mutate(predicted_price = predict(rf_model, newdata = data_holdout))

 

print(paste("RMSE value for the holdout dataset:",RMSE(data_holdout$predicted_price,data_holdout$price)))


```
Our RMSE on the training sample and the hold-out set are very similar. 

```{r, fig.height= 4, fig.width= 4, fig.pos = "center"}
ggplot(data = data_holdout, aes(x = price)) +
  geom_point(aes(y = predicted_price), colour = "red") +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed", size = 1.2) +
  xlim(0, 300) +
  ylim(0, 300) + 
  theme_bw()

```


## Extra Task 

### External validity with different dates

Our analysed dataset comes from a specific date in 13-12-2022 with data going back to one year. What is also available however, is a booking data of future dates, where we can test if our prediction models can predict the prices for the future. 

```{r message=FALSE, warning=FALSE, include=FALSE}
#Loading and cleaning the calendar data
calendar <- read.csv("calendar.csv")
calendar <- calendar%>%
  rename(
    id = listing_id,
    price_extra=price)%>%
  select(id, date, price_extra)
calendar$price_extra <- as.numeric(gsub("\\$", "", calendar$price_extra))


calendar <- calendar %>%
  group_by(id) %>%
  summarise(mean = mean(price_extra))


#Joining the data tables
data_extra <- left_join(dt, calendar, by = "id")
data_extra <- data_extra %>% rename( future_price = mean)

data_extra <- data_extra %>% mutate(OLS_pred = predict(cv5, newdata = data_extra),
                                    Lasso_pred = predict(lasso_model,newdata = data_extra),
                                    RF_pred = predict(rf_model,newdata = data_extra))


```

After joining our analyzed datatable with the future booking data, we introduced a variable 'future price'. Future price has 409 NAs, which were replaced with mean value.

```{r message=TRUE, warning=FALSE, include=FALSE}
data_extra <- data_extra %>%
  mutate(
    future_price = ifelse(is.na(data_extra$future_price),
                          mean(data_extra$future_price, na.rm = T), 
                          future_price))

summary(data_extra$future_price) 
```

When summarizing the data we can see that the RMSE is a lot higher. The order of the model fit did not change, however, random forest performs better than the previous two, with Lasso having slightly better results than OLS. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
futureOLS <-  RMSE(data_extra$OLS_pred,data_extra$future_price)
futureLASSO <- RMSE(data_extra$Lasso_pred,data_extra$future_price)
futureRF <- RMSE(data_extra$RF_pred,data_extra$future_price)

valid <- data.frame(
  Model = summary_all$Model,
  `Past Data RMSE` = summary_all$RMSE,
  `Future Data RMSE` = c(futureOLS, futureLASSO, futureRF)
)



knitr::kable(valid, caption = "Model comparison with future data", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position', full_width = F )
```



What skews our data are the listings with high prices, which are not frequent but take on extreme values. 

We can argue that while for the accuracy of future prediction it could have been better to keep our outliers in the very beginning, but when it comes to the business case, and on how to position the new apartments on the market, being mid-sized apartment without any special feature to our knowledge, it is best to keep our 95% threshold. The skewness also means that the future listings will also have outliers that might not be relevant to our business case. 

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4.5, fig.width= 4.5}
ggplot( data= data_extra, aes(x= future_price))+
  geom_point(aes(y= RF_pred),colour = 'red')+
  geom_abline(intercept = 0, slope = 1,color = 'black' ,linetype = 'dashed')+
  labs(titel = "Future prices vs prediction with Random forest", x= "Future price", y= "Predicted price")+
  theme_bw()
```

When comparing the prediction models we can see that all of our prediction models overestimate the distribution and the frequency around the mean value, but Random forest captures both the current and future distirbution visibly better.

```{r message=FALSE, warning=FALSE, include=FALSE}

data_extra2 <- data_extra %>% select(price, future_price, OLS_pred,Lasso_pred,RF_pred)
# Reshape the data into a long format
data_long <- gather(data_extra2, key = "Series", value = "Price", OLS_pred, Lasso_pred, RF_pred, price, future_price)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plot the data using ggplot2
ggplot(data = data_long, aes(x = Price, color = Series)) +
  geom_density(size = 1) +
  scale_color_manual(values = c("OLS_pred" = "orange", "Lasso_pred" = "blue", "RF_pred" = "red", "price" = "black", "future_price" = "green")) +
  labs(x = "", y = "Price") +
  theme_classic()
```


